{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KYC Funnel Analysis: Data Cleaning & Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview:\n",
    "- Purpose: find root causes for a drop in overall pass rate and investigate a reported 'impossible state' bug.\n",
    "- Steps: load data, clean and label common failure modes, create cohorts (esp. MEX + UNK), compute weekly metrics, and visualize trends and cohorts.\n",
    "- Notes: added binary flags for common issues (fraud, quality, tech failures) to simplify aggregation and correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, TargetEncoder\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyc_details = pd.read_csv('KYC_details.csv')\n",
    "kyc_summary = pd.read_csv('KYC_summary.csv')\n",
    "\n",
    "df = pd.merge(kyc_summary, kyc_details, on='user_reference', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_'] = pd.to_datetime(df['date_'])\n",
    "df['is_pass'] = df['decision_type'].isin(['PASSED', 'APPROVED']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = df[\n",
    "    (df['decision_type'].isin(['PASSED', 'APPROVED'])) & \n",
    "    (df['watchlist_screening_decision'].isna())\n",
    "]\n",
    "df_clean = df.drop(anomalies.index).copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_keywords = [\n",
    "    'DIGITAL_COPY', 'MANIPULATED', 'FAKE', 'PUNCHED', \n",
    "    'MISMATCH_FRONT_BACK', 'PHOTOCOPY'\n",
    "]\n",
    "fraud_pattern = '|'.join(fraud_keywords)\n",
    "\n",
    "df_clean['is_confirmed_fraud'] = (\n",
    "    df_clean['image_checks_decision_details'].astype(str).str.contains(fraud_pattern, case=False, na=False) |\n",
    "    df_clean['usability_decision_details'].astype(str).str.contains('PHOTOCOPY', case=False, na=False)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_keywords = [\n",
    "    'GLARE', 'BLURRED', 'MISSING_PAGE', 'NOT_UPLOADED', \n",
    "    'DAMAGED_DOCUMENT', 'PART_OF_DOCUMENT_MISSING', 'PART_OF_DOCUMENT_HIDDEN',\n",
    "    'BAD_QUALITY', 'FACE_NOT_FULLY_VISIBLE', 'liveness_UNDETERMINED'\n",
    "]\n",
    "quality_pattern = '|'.join(quality_keywords)\n",
    "\n",
    "df_clean['is_quality_fail'] = (\n",
    "    df_clean['usability_decision_details'].astype(str).str.contains(quality_pattern, case=False, na=False) |\n",
    "    df_clean['liveness_decision_details'].astype(str).str.contains(quality_pattern, case=False, na=False)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['is_face_mismatch'] = (df_clean['similarity_decision_details'] == 'NO_MATCH').astype(int)\n",
    "df_clean['is_unsupported_doc'] = df_clean['usability_decision_details'].isin(['UNSUPPORTED_DOCUMENT_TYPE', 'NOT_A_DOCUMENT']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_fail_keywords = ['TECHNICAL_ERROR', 'MISMATCHING_DATAPOINTS', 'MISMATCH_HRZ_MRZ_DATA']\n",
    "tech_pattern = '|'.join(tech_fail_keywords)\n",
    "\n",
    "df_clean['is_tech_data_fail'] = (\n",
    "    df_clean['data_checks_decision_details'].astype(str).str.contains(tech_pattern, case=False, na=False) |\n",
    "    df_clean['extraction_decision_details'].astype(str).str.contains(tech_pattern, case=False, na=False)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 18, 25, 35, 45, 55, 100]\n",
    "labels = ['<18', '18-25', '26-35', '36-45', '46-55', '55+']\n",
    "\n",
    "df_clean['proxy_age'] = 2023 - pd.to_numeric(df_clean['year_birth'], errors='coerce')\n",
    "df_clean['age_group'] = pd.cut(df_clean['proxy_age'], bins=bins, labels=labels).astype(str).replace('nan', 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_fill = ['data_issuing_country', 'data_type', 'data_sub_type']\n",
    "for col in cols_to_fill:\n",
    "    df_clean[col] = df_clean[col].fillna('UNKOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['combo_country_type'] = df_clean['data_issuing_country'] + \"_\" + df_clean['data_type']\n",
    "df_clean['combo_country_subtype'] = df_clean['data_issuing_country'] + \"_\" + df_clean['data_sub_type']\n",
    "df_clean['combo_country_age'] = df_clean['data_issuing_country'] + \"_\" + df_clean['age_group']\n",
    "df_clean['combo_type_age'] = df_clean['data_type'] + \"_\" + df_clean['age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_groups(df, feature):\n",
    "    stats = df.groupby(feature)['is_pass'].agg(['count', 'mean', 'sum']).reset_index()\n",
    "    stats.columns = [feature, 'Total Attempts', 'Pass Rate', 'Passed Count']\n",
    "    stats['Fail Count'] = stats['Total Attempts'] - stats['Passed Count']\n",
    "    stats = stats.sort_values('Pass Rate', ascending=True)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_stats_country_type = analyze_feature_groups(df_clean, 'combo_country_type')\n",
    "meaningful_country_type = group_stats_country_type[group_stats_country_type['Total Attempts'] > 10]\n",
    "meaningful_country_type.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_stats_country_age = analyze_feature_groups(df_clean, 'combo_country_age')\n",
    "meaningful_country_age = group_stats_country_age[group_stats_country_age['Total Attempts'] > 10]\n",
    "meaningful_country_age.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['is_mex_unk'] = (\n",
    "    (df_clean['data_issuing_country'] == 'MEX') & \n",
    "    (df_clean['data_type'] == 'UNKOWN')\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data = df_clean.groupby('week').agg({\n",
    "    'is_pass': 'mean',\n",
    "    'is_mex_unk': 'mean',\n",
    "    'is_quality_fail': 'mean',\n",
    "    'is_confirmed_fraud': 'mean',\n",
    "    'is_face_mismatch': 'mean',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = agg_data.corr()['is_pass'].drop('is_pass').sort_values()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_weekly = (agg_data - agg_data.min()) / (agg_data.max() - agg_data.min())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(normalized_weekly.index, normalized_weekly['is_pass'], linewidth=4, color='black', label='Pass Rate')\n",
    "\n",
    "top_suspect = \"is_mex_unk\"\n",
    "\n",
    "plt.plot(\n",
    "    normalized_weekly.index,\n",
    "    normalized_weekly[top_suspect],\n",
    "    linewidth=2, \n",
    "    linestyle='--',\n",
    "    color=\"red\",\n",
    "    label=f'{top_suspect} (Corr: {correlation_matrix[top_suspect]:.2f})'\n",
    ")\n",
    "\n",
    "plt.title('Trend Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['is_mex_unk_tech_fail'] = (\n",
    "    (df_clean['is_mex_unk'] == 1) & \n",
    "    (df_clean['is_tech_data_fail'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "df_clean['is_mex_unk_unssuported_document'] = (\n",
    "    (df_clean['is_mex_unk'] == 1) & \n",
    "    (df_clean['is_unsupported_doc'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "df_clean['is_mex_unk_quality_fail'] = (\n",
    "    (df_clean['is_mex_unk'] == 1) & \n",
    "    (df_clean['is_quality_fail'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "df_clean['is_mex_unk_mismatch'] = (\n",
    "    (df_clean['is_mex_unk'] == 1) & \n",
    "    (df_clean['is_face_mismatch'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "df_clean['is_mex_unk_fraud'] = (\n",
    "    (df_clean['is_mex_unk'] == 1) & \n",
    "    (df_clean['is_confirmed_fraud'] == 1)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_data_unkown = df_clean.groupby('week').agg({\n",
    "    'is_pass': 'mean',\n",
    "    'is_mex_unk_tech_fail': 'mean',\n",
    "    'is_mex_unk_unssuported_document': 'mean',\n",
    "    'is_mex_unk_quality_fail': 'mean',\n",
    "    'is_mex_unk_mismatch': 'mean',\n",
    "    'is_mex_unk_fraud': 'mean',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = weekly_data_unkown.corr()['is_pass'].drop('is_pass').sort_values()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mex_unk_cohort = df_clean[\n",
    "    (df_clean['is_mex_unk'] == 1)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mex_unk_cohort.groupby(\"usability_decision_details\")[\"user_reference\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mex_unk_cohort_bugs = mex_unk_cohort[\n",
    "    (mex_unk_cohort['usability_decision_details'] == 'OK')\n",
    "]\n",
    "\n",
    "mex_unk_cohort_bugs.groupby(\"image_checks_decision_details\")[\"user_reference\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_cohort = df_clean[\n",
    "    (df_clean['data_issuing_country'] == 'MEX') & \n",
    "    (df_clean['data_type'] == 'UNKOWN') & \n",
    "    (df_clean['usability_decision_details'] == 'OK')\n",
    "].copy()\n",
    "\n",
    "control_cohort = df_clean[\n",
    "    (df_clean['data_issuing_country'] == 'MEX') & \n",
    "    (~(df_clean['data_type'] == 'UNKOWN')) & \n",
    "    (df_clean['usability_decision_details'] == 'OK')\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_counts = bug_cohort['image_checks_decision_details'].value_counts()\n",
    "control_counts = control_cohort['image_checks_decision_details'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Users in Bug Cohort (MEX + UNK + Usability OK): {len(bug_cohort)}\")\n",
    "print(\"\\nImage Checks Outcome for Bug Cohort:\")\n",
    "print(bug_counts)\n",
    "\n",
    "print(f\"\\nTotal Users in Control Cohort (MEX + Known Doc + Usability OK): {len(control_cohort)}\")\n",
    "print(\"\\nImage Checks Outcome for Control Cohort (Top 5):\")\n",
    "print(control_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_impossible_rate = (bug_counts.get('PRECONDITION_NOT_FULFILLED', 0) / len(bug_cohort)) * 100\n",
    "control_impossible_rate = (control_counts.get('PRECONDITION_NOT_FULFILLED', 0) / len(control_cohort)) * 100\n",
    "\n",
    "print(f\"\\n% of Users getting 'PRECONDITION FAIL' after 'USABILITY OK':\")\n",
    "print(f\"  - Bug Cohort: {bug_impossible_rate:.2f}%\")\n",
    "print(f\"  - Control Cohort: {control_impossible_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_cohort['is_impossible'] = (bug_cohort['image_checks_decision_details'] == 'PRECONDITION_NOT_FULFILLED').astype(int)\n",
    "daily_bug_vol = bug_cohort.groupby('week')['is_impossible'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_bug_vol.index, daily_bug_vol.values, color='red', marker='o', linewidth=2, label='Impossible Cases (Usability OK -> Precondition Fail)')\n",
    "plt.title('Timeline of the \"Impossible State\" Bug (Daily Volume)')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.xlabel('Date')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the 'Impossible State' analysis:\n",
    "- Definition: we flag an \n",
    " when the issuing country is MEX, the detected document type is UNKOWN, and usability returned 'OK' â€” then later image checks produce a 'PRECONDITION_NOT_FULFILLED' (an impossible transition).\n",
    "- This block computes weekly rates and plots pass rate vs the impossible-bug rate to show correlation and timing.\n",
    "- Use this plot to correlate surges in this bug with drops in overall pass rate and to prioritize debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['is_impossible_bug'] = (\n",
    "    (df_clean['data_issuing_country'] == 'MEX') & \n",
    "    (df_clean['data_type'] == 'UNKOWN') & \n",
    "    (df_clean['usability_decision_details'] == 'OK')\n",
    ").astype(int)\n",
    "\n",
    "daily_stats = df_clean.groupby('week').agg({\n",
    "    'decision_type': lambda x: x.isin(['PASSED', 'APPROVED']).mean(),\n",
    "    'is_impossible_bug': 'mean'\n",
    "})\n",
    "daily_stats.columns = ['Pass_Rate', 'Impossible_Bug_Rate']\n",
    "\n",
    "daily_stats['Pass_Rate_Pct'] = daily_stats['Pass_Rate'] * 100\n",
    "daily_stats['Impossible_Bug_Rate_Pct'] = daily_stats['Impossible_Bug_Rate'] * 100\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "color_pass = 'black'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Overall Pass Rate', color=color_pass, fontweight='bold', fontsize=12)\n",
    "ax1.plot(daily_stats.index, daily_stats['Pass_Rate_Pct'], color=color_pass, linewidth=3, label='Pass Rate')\n",
    "ax1.tick_params(axis='y', labelcolor=color_pass)\n",
    "ax1.set_ylim(75, 90) \n",
    "ax1.grid(False)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color_bug = 'red'\n",
    "ax2.set_ylabel('State Bug Rate', color=color_bug, fontweight='bold', fontsize=12)\n",
    "ax2.plot(daily_stats.index, daily_stats['Impossible_Bug_Rate_Pct'], color=color_bug, linestyle='--', linewidth=2, marker='o', markersize=4, label='Impossible Bug (MEX UNK + Usability OK)')\n",
    "ax2.tick_params(axis='y', labelcolor=color_bug)\n",
    "ax2.set_ylim(3, 6)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.title('Pass Rate Drop vs. State Bug Surge', fontsize=16)\n",
    "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='center left')\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
